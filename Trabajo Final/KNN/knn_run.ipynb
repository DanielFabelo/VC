{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, cv2\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "\n",
    "directory_path = '../Dataset/'\n",
    "folders_list = []    \n",
    "    \n",
    "# Directory listing    \n",
    "for item in os.listdir(directory_path):\n",
    "    item_path = os.path.join(directory_path, item)\n",
    "    if os.path.isfile(item_path):\n",
    "        folders_list += [item]\n",
    "    elif os.path.isdir(item_path):\n",
    "        folders_list += [f'{item}:{len(os.listdir(item_path))}']        \n",
    "\n",
    "print(f'Folders: {folders_list}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "# Funzione per disegnare i landmarks sulla mano\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.multi_hand_landmarks  # Lista dei punti chiave della mano\n",
    "    annotated_image = np.copy(rgb_image)  # Crea una copia dell'immagine originale\n",
    "\n",
    "    for hand_landmarks in hand_landmarks_list:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,  # Immagine su cui disegnare\n",
    "            hand_landmarks,  # Punti chiave da disegnare\n",
    "            mp.solutions.hands.HAND_CONNECTIONS  # Connessioni tra i punti chiave\n",
    "        )\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "# Inizializza MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Cattura il video dalla fotocamera (cambia '0' per webcam diverse)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Converti l'immagine in formato RGB\n",
    "    rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Rileva i punti chiave delle mani\n",
    "    results = hands.process(rgb_image)\n",
    "\n",
    "    # Se ci sono mani rilevate, disegna i landmarks\n",
    "    if results.multi_hand_landmarks:\n",
    "        annotated_image = draw_landmarks_on_image(frame, results)\n",
    "    else:\n",
    "        annotated_image = frame  # Usa il frame originale se non ci sono mani\n",
    "\n",
    "    # Mostra l'immagine con i landmarks\n",
    "    cv2.imshow('Hand Landmarks', annotated_image)\n",
    "\n",
    "    # Uscita premendo 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "# Percorso al dataset\n",
    "dataset_path = '../Dataset/'\n",
    "\n",
    "# Inizializza MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7)\n",
    "\n",
    "# File CSV per salvare le feature\n",
    "csv_file = '../KNN/asl_features.csv'\n",
    "\n",
    "with open(csv_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Itera attraverso ogni cartella del dataset (lettere dalla 'a' alla 'z')\n",
    "    for label in sorted(os.listdir(dataset_path)):\n",
    "        folder_path = os.path.join(dataset_path, label)\n",
    "        if os.path.isdir(folder_path):\n",
    "            for img_file in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_file)\n",
    "                image = cv2.imread(img_path)\n",
    "\n",
    "                # Converti l'immagine in RGB\n",
    "                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                # Rileva i landmarks\n",
    "                results = hands.process(rgb_image)\n",
    "                if results.multi_hand_landmarks:\n",
    "                    # Estrai le feature dai landmarks\n",
    "                    for hand_landmarks in results.multi_hand_landmarks:\n",
    "                        features = []\n",
    "                        for landmark in hand_landmarks.landmark:\n",
    "                            features.extend([landmark.x, landmark.y, landmark.z])\n",
    "                        # Aggiungi l'etichetta della lettera\n",
    "                        features.append(label)\n",
    "                        writer.writerow(features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Suddivisione del dataset in Training e Test Set #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carica il dataset\n",
    "import pandas as pd\n",
    "data = pd.read_csv('../KNN/asl_features.csv', header=None)\n",
    "X = data.iloc[:, :-1].values  # Features\n",
    "y = data.iloc[:, -1].values   # Etichette (lettere)\n",
    "\n",
    "# Dividi il dataset (80% training, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ricerca del valore ottimale di k #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Prova diversi valori di k\n",
    "k_values = range(1, 21)\n",
    "mean_accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5)  # 5-fold cross-validation\n",
    "    mean_accuracies.append(scores.mean())\n",
    "    print(f'k={k}, Accuracy: {scores.mean():.4f}')\n",
    "\n",
    "# Trova il valore di k con la migliore accuratezza\n",
    "best_k = k_values[np.argmax(mean_accuracies)]\n",
    "print(f'Miglior valore di k: {best_k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Valutazione del modello con il valore k trovato #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inizializza il modello con il miglior valore di k\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Valutazione sul test set\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(f'Accuratezza sul test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Riconoscimento dopo aver cliccato space #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danif\\anaconda3\\envs\\ASL\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator KNeighborsClassifier from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognition activated\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Carica il modello KNN addestrato\n",
    "with open('../Models/knn_model_asl.pkl', 'rb') as f:\n",
    "    knn = pickle.load(f)\n",
    "\n",
    "# Inizializza MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Funzione per normalizzare i landmarks\n",
    "def normalize_features(hand_landmarks):\n",
    "    x_coords = [landmark.x for landmark in hand_landmarks.landmark]\n",
    "    y_coords = [landmark.y for landmark in hand_landmarks.landmark]\n",
    "\n",
    "    # Calcola il bounding box della mano\n",
    "    min_x, max_x = min(x_coords), max(x_coords)\n",
    "    min_y, max_y = min(y_coords), max(y_coords)\n",
    "\n",
    "    # Normalizzazione rispetto al bounding box\n",
    "    normalized_features = []\n",
    "    for landmark in hand_landmarks.landmark:\n",
    "        norm_x = (landmark.x - min_x) / (max_x - min_x)\n",
    "        norm_y = (landmark.y - min_y) / (max_y - min_y)\n",
    "        normalized_features.extend([norm_x, norm_y, landmark.z])  # Puoi includere o escludere Z a seconda dei test\n",
    "    return normalized_features\n",
    "\n",
    "# Variabile per abilitare/disabilitare la predizione\n",
    "recognition_active = False\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    # Converti l'immagine in RGB per Mediapipe\n",
    "    rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_image)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Disegna i landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            if recognition_active:\n",
    "                # Estrai le feature e fai la predizione\n",
    "                features = []\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    features.extend([landmark.x, landmark.y, landmark.z])\n",
    "                prediction = knn.predict([features])\n",
    "                # Visualizza la lettera predetta\n",
    "                cv2.putText(frame, f'Predicted: {prediction[0]}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Mostra il frame con o senza la predizione\n",
    "    cv2.imshow('Sign Language Recognition', frame)\n",
    "\n",
    "    # Gestione dei tasti\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):  # Esci dal programma\n",
    "        break\n",
    "    elif key == ord(' '):  # Attiva/disattiva il riconoscimento con barra spaziatrice\n",
    "        recognition_active = not recognition_active\n",
    "        print(f'Recognition {\"activated\" if recognition_active else \"deactivated\"}')\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
